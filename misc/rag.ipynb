{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"Which champions are from Shurima\"],\n",
    "    n_results=1,\n",
    "    include=['documents']\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CHUNKING LOGIC\n",
    "import csv\n",
    "from time import sleep\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "index_name = \"lolbot\"\n",
    "\n",
    "load_dotenv()\n",
    "RAG_PINECONE_API_KEY = os.getenv(\"RAG_PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=RAG_PINECONE_API_KEY)\n",
    "sleep(1)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "            )\n",
    "        )\n",
    "else:\n",
    "    print(\"Pinecone index already exists\")\n",
    "\n",
    "with open('league_lore2.csv', 'r', encoding = 'utf8') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    championdata = [row for row in csv_reader]\n",
    "    \n",
    "chunk_size = 300\n",
    "chunk_overlap = 20\n",
    "\n",
    "def chunk_section(championdata, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    chunks_list = []\n",
    "    for champion_entry in championdata:\n",
    "        chunks = text_splitter.create_documents(\n",
    "            texts=[champion_entry[\"Lore\"]],\n",
    "            metadatas=[{\"Link\": champion_entry[\"Link\"], \"Champion\": champion_entry[\"Champion\"], \"Region\": champion_entry[\"Region\"]}],\n",
    "        )\n",
    "        chunks_list.extend([{\"Link\": champion_entry[\"Link\"], \"Lore\": chunk.page_content, \"Champion\": chunk.metadata[\"Champion\"], \"Region\": chunk.metadata[\"Region\"]} for chunk in chunks])\n",
    "\n",
    "\n",
    "    return chunks_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Chunk the data\n",
    "chunks = chunk_section(championdata, chunk_size, chunk_overlap)\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to embed texts using SentenceTransformer\n",
    "def embed_texts(texts):\n",
    "    embeddings = embedder.encode(texts, convert_to_tensor=True)\n",
    "    return embeddings\n",
    "\n",
    "# Embed the chunked texts\n",
    "chunk_texts = [chunk[\"Lore\"] for chunk in chunks]\n",
    "embeddings = embed_texts(chunk_texts)\n",
    "\n",
    "# Print to debug the embedding dimension\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Prepare data for insertion into Pinecone\n",
    "pinecone_data = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    pinecone_data.append({\n",
    "        \"id\": f\"{chunk['Champion']}_{i}\",  # Unique ID for each chunk\n",
    "        \"values\": embeddings[i].tolist(),\n",
    "        \"metadata\": {\n",
    "            \"Link\": chunk[\"Link\"],\n",
    "            \"Champion\": chunk[\"Champion\"],\n",
    "            \"Region\": chunk[\"Region\"],\n",
    "            \"Lore\": chunk[\"Lore\"]\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Debug to check the first item to be upserted\n",
    "print(\"First item to be upserted:\", pinecone_data[0])\n",
    "\n",
    "# Insert data into Pinecone\n",
    "try:\n",
    "    index.upsert(vectors=pinecone_data, namespace='test')\n",
    "except Exception as e:\n",
    "    print(f\"Error during upsert: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Function to query Pinecone and retrieve relevant chunks\n",
    "def query_pinecone(index, query_text, top_k=5):\n",
    "    query_embedding = embed_texts([query_text])[0].tolist()  # Embed the query using the same model\n",
    "    result = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)  # Query Pinecone with the query embedding\n",
    "    return result\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Function to generate a response from the GPT-2 model\n",
    "def generate_response(context, query, model, tokenizer):\n",
    "    input_text = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=150,  # Adjust as necessary\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"Answer:\")[1].strip()\n",
    "\n",
    "# Example query\n",
    "query_text = \"Who is the darkin blade.\"\n",
    "\n",
    "# Retrieve relevant chunks from Pinecone\n",
    "results = query_pinecone(index, query_text)\n",
    "\n",
    "# Combine the content of the retrieved chunks\n",
    "combined_context = \"\\n\\n\".join([match['metadata']['Lore'] for match in results['matches']])\n",
    "\n",
    "# Generate a response using the combined context and the query\n",
    "response = generate_response(combined_context, query_text, model, tokenizer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST CHUNKING SIZE AND OVERLAP\n",
    "import csv\n",
    "\n",
    "# Define the chunk size and overlap\n",
    "chunk_size = 300\n",
    "chunk_overlap = 20\n",
    "\n",
    "# Open and read the CSV file\n",
    "with open('league_lore.csv', 'r', encoding='utf8') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    championdata = [row for row in csv_reader]\n",
    "\n",
    "# Call the chunk_section method\n",
    "chunks_result = chunk_section(championdata, chunk_size, chunk_overlap)\n",
    "\n",
    "# Print the resulting chunks\n",
    "for chunk in chunks_result:\n",
    "    print(chunk)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
