{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone index already exists\n"
     ]
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "RAG_PINECONE_API_KEY = os.getenv(\"RAG_PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=RAG_PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"lore-bot\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        vector_type=\"dense\",\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        ),\n",
    "        deletion_protection=\"disabled\",\n",
    "        tags={\n",
    "            \"environment\": \"development\"\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    print(\"Pinecone index already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=RAG_PINECONE_API_KEY)\n",
    "\n",
    "index_list = pc.list_indexes()\n",
    "\n",
    "print(index_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/league_lore_df.csv')\n",
    "dataset = df.drop('Unnamed: 4', axis=1)\n",
    "\n",
    "print(dataset.to_string()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, rows in dataset.iterrows():\n",
    "    print(rows['Lore'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Champion</th>\n",
       "      <th>Region</th>\n",
       "      <th>Lore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>https://universe.leagueoflegends.com/en_AU/sto...</td>\n",
       "      <td>zyra</td>\n",
       "      <td>Ixtal</td>\n",
       "      <td>Zyra’s memory is long, and runs as deep as the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Link Champion Region  \\\n",
       "164  https://universe.leagueoflegends.com/en_AU/sto...     zyra  Ixtal   \n",
       "\n",
       "                                                  Lore  \n",
       "164  Zyra’s memory is long, and runs as deep as the...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testSet contains only the last champion (Zyra) entry as its entire dataset\n",
    "\n",
    "testSet = dataset.copy()\n",
    "for i in range(164):\n",
    "    testSet.drop(i, inplace=True)\n",
    "testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert Debugging / Testing\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "text_splitter = SemanticChunker(embedding_model)\n",
    "\n",
    "index = pc.Index(\"lore-bot\")\n",
    "unique_id = 0\n",
    "\n",
    "# Iterates through the test dataset and stores the metadata of the current row (a.k.a champion)\n",
    "for _, rows in testSet.iterrows():\n",
    "    link = rows['Link']\n",
    "    champion  = rows['Champion']\n",
    "    region = rows['Region']\n",
    "    lore = rows['Lore']\n",
    "\n",
    "    # Chunks the lore section of the champion using SemanticChunker\n",
    "    chunks = text_splitter.split_text(lore)\n",
    "\n",
    "    # Uses the same model to embed the chunks, preparing for upsert\n",
    "    for c in chunks:\n",
    "        vector = embedding_model.embed_documents([c])\n",
    "        metadata = {\n",
    "            \"champion\": champion,\n",
    "            \"region\": region,\n",
    "            \"link\": link,\n",
    "            \"lore\": c\n",
    "        }\n",
    "        unique_id += 1\n",
    "        index.upsert(vectors=[{\"id\": str(unique_id), \"values\": vector[0], \"metadata\": metadata}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy.py test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieval\n",
    "    \"\"\"\n",
    "    # Query the database\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "    index = pc.Index(\"lore-bot\")\n",
    "\n",
    "    # Embed the query with the same model that embeds the lore vectors\n",
    "    # query = \"Who is Zyra\"\n",
    "    query_vector = embedding_model.embed_query(query)\n",
    "\n",
    "    # Retrieve the list of response vectors that are most similar to the query vector\n",
    "    results = index.query(vector=query_vector, top_k=1, include_metadata=True)\n",
    "\n",
    "    # Store the lore data of each result vector into a list\n",
    "    retrieved_chunks = []\n",
    "\n",
    "    for match in results['matches']:\n",
    "        retrieved_chunks.append(match['metadata']['lore'])\n",
    "\n",
    "    return results['matches'][0]['metadata']['lore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zyra’s memory is long, and runs as deep as the roots of the earth. Her kind was young when the Rune Wars raged, when mortal armies fought one another for the very keys of creation. Hidden in the jungles south of Kumungu, somewhere between the great rivers that divide eastern Shurima, lay the fabled Gardens of Zyr. Elemental magics had turned the soil there in strange and unpredictable ways, giving rise to fierce, carnivorous plants that preyed upon any creature that strayed within reach. They infested and they devoured, caring nothing for the squabbles of mortals, content merely to coil their vines through the forests and swamplands. In their own way, they were all Zyra… and nourishment was plentiful, even in the midst of war. A small company of soldiers, their allegiance long since lost to time, advanced through those lands in search of some now-forgotten prize. They were led by an ambitious sorceress—but they were far from home, bound to succumb to the noxious fumes and spores of that accursed place. The denizens of the Gardens set upon them, spined tendrils lashing through armor and flesh with sadistic ease. Though they fought valiantly, the warriors knew they could not hold out long, and turned to their sorceress to save them. Gathering her powers, she wrought a mighty blast.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"Tell me about Zyra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the database\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "index = pc.Index(\"lore-bot\")\n",
    "\n",
    "# Embed the query with the same model that embeds the lore vectors\n",
    "query = \"Who is Zyra\"\n",
    "query_vector = embedding_model.embed_query(query)\n",
    "\n",
    "# Retrieve the list of response vectors that are most similar to the query vector\n",
    "results = index.query(vector=query_vector, top_k=5, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the lore data of each result vector into a list\n",
    "retrieved_chunks = []\n",
    "\n",
    "for match in results['matches']:\n",
    "    retrieved_chunks.append(match['metadata']['lore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieved_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate each lore chunk using \\n\\n so the LLM can understand where one chunk ends and where one begins\n",
    "\n",
    "context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "prompt = f\"\"\"Answer the following question about League of Legends champion lore based on the provided context. Be accurate and concise.\n",
    "\n",
    "Context:{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Generate the answer\n",
    "response = generator(prompt, max_new_tokens=512)\n",
    "print(response[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_chunks = []\n",
    "# if results['matches']:\n",
    "#     first_vector = results['matches'][0]['metadata']['lore']\n",
    "#     context_chunks.append(first_vector)\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Prompt Engineering\n",
    "# \"\"\"\n",
    "# # Separate each lore chunk using \\n\\n so the LLM can understand where one chunk ends and where one begins\n",
    "\n",
    "# context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "# prompt = f\"\"\"Answer the following question about League of Legends champion lore based on the provided context. Be accurate and concise.\n",
    "\n",
    "# Context:{context}\n",
    "\n",
    "# Question: {query}\n",
    "\n",
    "# Answer:\"\"\"\n",
    "\n",
    "# generator = pipeline(\"text-generation\", model=\"BAAI/bge-base-en-v1.5\", max_length=512, truncation=True)\n",
    "\n",
    "# # Generate the answer\n",
    "# response = generator(prompt, max_new_tokens=512)\n",
    "# return response[0]['generated_text']\n",
    "\n",
    "# \"\"\"\n",
    "# Generation\n",
    "# \"\"\"\n",
    "\n",
    "# ###\n",
    "# # Initialize Flan-T5 for generation (best for lore Q&A)\n",
    "# # Use flan-t5-large for better quality\n",
    "# # Adjust based on your character limit\n",
    "# # Lower temperature for more factual responses\n",
    "# ###\n",
    "# generator = pipeline(\n",
    "#     \"text2text-generation\",\n",
    "#     model=\"google/flan-t5-base\",  \n",
    "#     max_length=300,  \n",
    "#     do_sample=True,\n",
    "#     temperature=0.3,  \n",
    "#     early_stopping=True\n",
    "# )\n",
    "\n",
    "# # Step 4: Generate response\n",
    "# response = generator(prompt, max_length=200, num_return_sequences=1)\n",
    "# return response[0]['generated_text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
